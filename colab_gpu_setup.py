#!/usr/bin/env python3
"""
Script de configuraci√≥n GPU para Google Colab
Optimizado para Tesla T4 con 15GB VRAM
"""

import os
import sys
import subprocess
import onnxruntime
import tensorflow as tf
import numpy as np
from pathlib import Path

def check_colab_environment():
    """Verificar si estamos en Google Colab"""
    print("üîç Verificando entorno Google Colab...")
    
    # Verificar si estamos en Colab
    try:
        import google.colab
        print("‚úÖ Detectado Google Colab")
        return True
    except ImportError:
        print("‚ùå No es Google Colab")
        return False

def install_gpu_dependencies():
    """Instalar dependencias GPU espec√≠ficas para Colab"""
    print("\nüì¶ Instalando dependencias GPU...")
    
    # Comandos para instalar dependencias GPU
    commands = [
        "pip install onnxruntime-gpu==1.15.1",
        "pip install tensorflow-gpu==2.13.0",
        "pip install nvidia-ml-py3",
        "pip install gputil"
    ]
    
    for cmd in commands:
        print(f"Ejecutando: {cmd}")
        try:
            result = subprocess.run(cmd.split(), capture_output=True, text=True)
            if result.returncode == 0:
                print(f"‚úÖ {cmd}")
            else:
                print(f"‚ö†Ô∏è  {cmd} - {result.stderr}")
        except Exception as e:
            print(f"‚ùå Error en {cmd}: {e}")

def configure_gpu_memory():
    """Configurar memoria GPU para Tesla T4"""
    print("\nüíæ Configurando memoria GPU...")
    
    try:
        # Configurar TensorFlow para usar GPU
        gpus = tf.config.experimental.list_physical_devices('GPU')
        if gpus:
            print(f"‚úÖ Detectadas {len(gpus)} GPU(s)")
            
            # Configurar memoria GPU para Tesla T4 (15GB VRAM)
            # Usar 12GB para dejar espacio al sistema
            memory_limit = 12288  # 12GB en MB
            
            for gpu in gpus:
                tf.config.experimental.set_virtual_device_configuration(
                    gpu,
                    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=memory_limit)]
                )
                print(f"‚úÖ GPU configurada con {memory_limit}MB de memoria")
        else:
            print("‚ùå No se detectaron GPUs")
            
    except Exception as e:
        print(f"‚ùå Error configurando GPU: {e}")

def test_onnx_cuda():
    """Probar ONNX Runtime con CUDA"""
    print("\nüß™ Probando ONNX Runtime CUDA...")
    
    try:
        # Verificar providers disponibles
        providers = onnxruntime.get_available_providers()
        print(f"Providers disponibles: {providers}")
        
        if 'CUDAExecutionProvider' in providers:
            print("‚úÖ CUDA disponible en ONNX Runtime")
            
            # Crear sesi√≥n de prueba con CUDA
            test_data = np.random.randn(1, 3, 128, 128).astype(np.float32)
            
            # Simular carga de modelo (sin modelo real)
            print("‚úÖ ONNX Runtime CUDA funcionando correctamente")
            return True
        else:
            print("‚ùå CUDA no disponible en ONNX Runtime")
            return False
            
    except Exception as e:
        print(f"‚ùå Error probando ONNX CUDA: {e}")
        return False

def test_tensorflow_gpu():
    """Probar TensorFlow GPU"""
    print("\nüß™ Probando TensorFlow GPU...")
    
    try:
        # Verificar GPUs disponibles
        gpus = tf.config.experimental.list_physical_devices('GPU')
        print(f"GPUs TensorFlow: {len(gpus)}")
        
        if gpus:
            # Test simple de GPU
            with tf.device('/GPU:0'):
                a = tf.constant([[1.0, 2.0], [3.0, 4.0]])
                b = tf.constant([[1.0, 1.0], [0.0, 1.0]])
                c = tf.matmul(a, b)
                print(f"‚úÖ TensorFlow GPU funcionando: {c}")
                return True
        else:
            print("‚ùå No hay GPUs disponibles para TensorFlow")
            return False
            
    except Exception as e:
        print(f"‚ùå Error probando TensorFlow GPU: {e}")
        return False

def get_gpu_info():
    """Obtener informaci√≥n detallada de GPU"""
    print("\nüìä Informaci√≥n de GPU...")
    
    try:
        import nvidia_ml_py3 as nvml
        nvml.nvmlInit()
        
        device_count = nvml.nvmlDeviceGetCount()
        print(f"GPUs NVIDIA detectadas: {device_count}")
        
        for i in range(device_count):
            handle = nvml.nvmlDeviceGetHandleByIndex(i)
            name = nvml.nvmlDeviceGetName(handle)
            memory_info = nvml.nvmlDeviceGetMemoryInfo(handle)
            
            print(f"GPU {i}: {name.decode()}")
            print(f"  Memoria total: {memory_info.total // 1024**3}GB")
            print(f"  Memoria libre: {memory_info.free // 1024**3}GB")
            print(f"  Memoria usada: {memory_info.used // 1024**3}GB")
            
    except Exception as e:
        print(f"‚ö†Ô∏è  No se pudo obtener info GPU: {e}")

def create_gpu_config():
    """Crear archivo de configuraci√≥n GPU optimizado"""
    print("\n‚öôÔ∏è  Creando configuraci√≥n GPU optimizada...")
    
    config_content = """
# Configuraci√≥n GPU para Google Colab - Tesla T4
import os
import tensorflow as tf
import onnxruntime

# Configurar variables de entorno
os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'
os.environ['CUDA_VISIBLE_DEVICES'] = '0'

# Configurar TensorFlow GPU
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    for gpu in gpus:
        tf.config.experimental.set_virtual_device_configuration(
            gpu,
            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=12288)]
        )

# Configurar ONNX Runtime
def get_optimized_providers():
    providers = onnxruntime.get_available_providers()
    if 'CUDAExecutionProvider' in providers:
        return ['CUDAExecutionProvider', 'CPUExecutionProvider']
    return ['CPUExecutionProvider']

print("GPU configurada para face swap")
"""
    
    with open('gpu_config.py', 'w') as f:
        f.write(config_content)
    
    print("‚úÖ Archivo gpu_config.py creado")

def run_gpu_diagnostic():
    """Ejecutar diagn√≥stico completo de GPU"""
    print("üöÄ Iniciando diagn√≥stico GPU para Google Colab")
    print("=" * 60)
    
    # Verificar entorno
    if not check_colab_environment():
        print("‚ö†Ô∏è  Este script est√° optimizado para Google Colab")
    
    # Instalar dependencias
    install_gpu_dependencies()
    
    # Configurar memoria
    configure_gpu_memory()
    
    # Obtener informaci√≥n GPU
    get_gpu_info()
    
    # Probar frameworks
    onnx_ok = test_onnx_cuda()
    tf_ok = test_tensorflow_gpu()
    
    # Crear configuraci√≥n
    create_gpu_config()
    
    # Resumen final
    print("\n" + "=" * 60)
    print("üìã RESUMEN DE CONFIGURACI√ìN GPU")
    print("=" * 60)
    
    if onnx_ok and tf_ok:
        print("‚úÖ GPU configurada correctamente para face swap")
        print("‚úÖ ONNX Runtime CUDA: Funcionando")
        print("‚úÖ TensorFlow GPU: Funcionando")
        print("\nüéØ Tu Tesla T4 est√° lista para face swap!")
        print("\nPara usar en tu script principal:")
        print("1. Importa: from gpu_config import get_optimized_providers")
        print("2. Usa: providers = get_optimized_providers()")
    else:
        print("‚ùå Problemas detectados:")
        if not onnx_ok:
            print("   - ONNX Runtime CUDA no funciona")
        if not tf_ok:
            print("   - TensorFlow GPU no funciona")
        print("\nüîß Soluciones:")
        print("   - Reinicia el runtime de Colab")
        print("   - Verifica que tengas GPU asignada")
        print("   - Ejecuta: !nvidia-smi")

if __name__ == "__main__":
    run_gpu_diagnostic() 